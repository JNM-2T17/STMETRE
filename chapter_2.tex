%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Filename    : chapter_2.tex 
%
%   Description : This file will contain your Review of Related Literature.
%                 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Review of Related Literature}
\label{sec:relatedlit}

This chapter discusses the features, capabilities, and limitations of existing research, algorithms, or software that are related or similar to the proposed research. This chapter is divided into three sections. The first section discusses algorithms for community detection. The second section discusses algorithms for sentiment analysis and other similarity parameters used for community detection. The third section discusses community evaluation metrics.

\begin{comment}
%
% IPR acknowledgement: the contents withis this comment are from Ethel Ong's slides on RRL.
%
Guide on Writing your RRL chapter

1. Identify the keywords with respect to your research
One keyword = One document section
Examples: 2.1 Story Generation Systems
2.2 Knowledge Representation

2.  Find references using these keywords

3.  For each of the references that you find,
Check: Is it relevant to your research?
Use their references to find more relevant works.

4. Identify a set of criteria for comparison.
It will serve as a guide to help you focus on what to look for

5. Write a summary focusing on -
What: A short description of the work
How: A summary of the approach it utilized
Findings: If applicable, provide the results
Why: Relevance to your work

6. At the end of each section,  show a Table of Comparison of the related works 
and your proposed project/system

\end{comment}

\section{Community Detection}

\citeA{Tang:2010} wrote a textbook on community detection. In the book, they sought to introduce characteristics of social media, review representative tasks of computing with social media, and illustrate associated challenges because with the emergence of social media websites, they felt it was an avenue to study human interaction and collaboration on an unparalleled scale. Multiple community detection approaches were discussed such as node-centric, network-centric, and hierarchy-centric. 

Node-centric algorithms involve the maximum clique detection problem which involves searching for a maximum complete subgraph of nodes in a network graph that are all adjacent to each other. The clique percolation method can find overlapping communities by finding cliques of size k, and then producing a clique graph, wherein two cliques are connected if they share k-1 nodes. Each connected element in this clique graph is then a community.

Network-centric algorithms involves vertex similarity, which is the similarity of the node’s social circles or how many common friends the two nodes have. This is what structural equivalence deals with. Two nodes $v_i$ and $v_j$ are structurally equivalent if $\forall v_k \in \{x \mid x \neq v_i \wedge x \neq v_j \}$ $e(v_i,v_k) \in E \iff e(v_j,v_k) \in E$, which is to say, $v_i$ and $v_j$ are connected to the exact same nodes. Nodes that are structurally equivalent belong to a community. 

Hierarchy-centric algorithms come in two forms: divisive and agglomerative. In divisive clustering, the entire set of nodes starts out in one set and each time, each set is divided into two until each community only has one member. The division is done by finding the node with the lowest edge betweenness and removing it, since that node is most likely the node connecting two communities. Agglomerative clustering starts with each node in their own community and communities are joined if they increase the overall modularity of the set of communities. Modularity is given by

\begin{equation}
Q = \frac{1}{2m} \sum_{l = 1}^{k} \sum_{i \in C_l, j \in C_l} (A_{ij} - \frac{d_i d_j}{2m})
\end{equation}

Where $m$ is the number of edges, $d_i$ is the degree of node $v_i$, $C_l$ being the $lth$ community, and $A_ij$ being the value in the adjacency matrix for node $v_i$ and $v_j$.

These algorithms may be considered in the final community detection phase of the proposed research \cite{Tang:2010}.

\citeA{Lim:2012:1} aimed to detect communities that share common interests on Twitter, based on linkages among followers of celebrities representing an interest category because they wish to help markets identify target groups with common interests. However, their approach differs from the typical  paradigm of ''identify communities then, for each, identify interests'', for they first identified interests they wish to extract communities from and from these interests, they then extracted the communities. 

Given a set of celebrities, C, celebrities being users with more than ten thousand followers, the algorithm first gets the common followers of all the celebrities in the set using the formula. 

\begin{equation}
P = \bigcap_{j \in C} (\bigcup_i link(i,c_j))
\end{equation}

Where $link(i,j)$ is given by

\begin{equation}
link(i,j) = \begin{cases}
\{i\} & \text{i follows j} \\
\emptyset & \text{i does not follow j}
\end{cases}
\end{equation}

Given this set of fans, P, they used the Infomap Algorithm and the Clique Percolation Method to detect communities in P. For each interest they wish to extract communities for, they chose the top 6 most popular celebrities based on follower count. Google and Wikipedia were used to identify which interests a celebrity represents. Afterwards, all users that follow the 6 celebrities were selected. They then selected 200,858 random users as a control group. Their algorithm produced more communities and larger communities than the control group, as well as more consistent communities, having a higher clustering coefficient. 

This paper provides an interesting alternative to detect communities by first specifying the interest of the community before detecting it, which may be used in the proposed research on top of the usual algorithms \cite{Lim:2012:1}.


\citeA{Zhang:2012} sought to identify communities in Twitter based on common interests. This is because Twitter has become very popular recently but little is known of it in the user level. This study would help in user recommendation and tweet recommendation as well as viral marketing to specific target groups. To identify the communities, they first compute specific feature similarities, then aggregate these features to compute for the final user similarity, and then they used classical clustering algorithms to detect the communities. To identify the communities, they first compute specific feature similarities, then aggregate these features to compute for the final user similarity, and then they used classical clustering algorithms to detect the communities.

The specific features they used were textual contents. Each data point was the entirety of a user’s tweets. Latent Dirichlet Allocation was used to identify latent topics from the user’s tweets. URL similarity was also detected, finding which users share similar links. Hashtag similarity was also analyzed. The social structure of users was also analyzed, which includes following similarity and retweeting similarity. 

In aggregating these similarities, the weighted sum of the previous similarities was computed to get the final similarity. Finally, k-means clustering was used to detect the communities based on their computed similarities. 

This paper presents a possible framework for detecting the communities. The proposed research may even use similar features, in addition to the Facebook specific features, to detect similarity. The k-means clustering algorithm will be one of the proposed algorithms for use in the proposed research \cite{Zhang:2012}.

\citeA{Deitrick:2013} sought to use sentiment classification to analyze communities in Twitter because harvesting information from these online social networks (OSN) would aid in the fields of politics and marketing. 

Their process is as follows: The follower network was represented as a weighted directed graph, each with initial weight of 1. To augment this, replies, mentions, retweets, hashtags, and sentiment classification of tweets were also harvested. These factors adjusted the weights in the graph. For community detection, the Infomap algorithm and Speaker-listener Label Propagation Algorithm(SLPA) were run. 

Generally, the network with updated weights produced communities with greater modularity. Of the two algorithms, the Infomap algorithm performed better. Recurring sentiment analysis was also helped by performing the aforementioned algorithms on the accounts that have already been placed in detected communities, which permits more in-depth analysis into the user’s sentiment since it could be analyzed within the context of the detected community.

This research provides a new way to represent the network, with their updated weights, as well as more possible algorithms to consider in detecting communities \cite{Deitrick:2013}.

\citeA{Cao:2015} proposed a visual analysis system, SocialHelix, because social media is a grand avenue for people to express their opinions and the researchers believed that an intuitive visualization that unfolds the process of sentiment divergence would have a far-reaching impact on multiple domains. 

They first identified the key domain problems of social divergence before employing a data abstraction design to convert the raw data into a form that captures all the key factors of the aforementioned domain problems. This abstracted data is then represented in a visualization based on a visual DNA metaphor. In identifying the key domain problems, it is determined when divergences start and end, how they evolve, who is involved, what roles do they play, and why does divergence occur. In the abstraction phase, the raw data is decomposed into temporal extent of social communities, topics or events, and user responses to these topics or events. In the visualization phase, the opposite sides of the helix represent the two sides of a divergence. The helix curves represents the changes in the communities’ sentiment. Nucleobase pairs represents events that connect the two communities. 

In implementation, the data was first filtered, removing unrelated posts and people. Statistical Linguistic Sentiment Analysis was used to determine the user’s sentiment. Finally, clustering was done using Hadoop, producing a cluster with 30 nodes. 

In the end, all test users were impressed by the visualization and agreed with the researchers’ model for the visualization. All test users felt that divergence identification was made easy due to the visualization. 

This research gives a sample visualization that can inspire the proponent’s own visualization, albeit the goal is not to model single divergences but an entire community. A possible tool, Hadoop, is also mentioned, which may be used to cluster the data \cite{Cao:2015}.

\section{Similarity Parameters}
This section outlines the basis/features/parameters used in detecting communities. It is divided into two subsections. The first subsection deals solely with sentiment analysis. The second subsection deals with other network and node parameters not related to sentiment analysis. 

\subsection{Sentiment Analysis}

\citeA{Zhang:2012} provided a formula to determine similarity in terms of text. 

\begin{equation}
sim_{text}(i,j) = \frac{1}{\sqrt{D_{js}(i,j)}}
\end{equation}

$D_{js}$ is the Jensen-Shannon Divergence between the two user’s topic probability distribution given by

\begin{equation}
D_{js}(i,j) = \frac{D_{kl}(UT_i \mid\mid M) + D_{kl}(UT_j \mid\mid M)}{2}
\end{equation}

Where $M = \frac{UT_i + UT_j}{2}$ and $D_{kl}(P \mid\mid Q) = \sum_{i \in topics} P(i) \log{\frac{P(i)}{Q(i)}}$ and $UT_i$ is the probability distribution of user i for all topics. $UT_i[t]$ is the probability distribution for user i on topic t.

This research provides a metric to determine the similarity of two users in terms of post content, which can be used in the proposed research \cite{Zhang:2012}.

\citeA{Deitrick:2013} used a subjective/objective and positive/negative Naive Bayes classifier. To do this, all tweets were converted to lowercase; hashtags, usernames, urls were replaced with twitterhashtag, twitterusername, and twitterurl respectively; the tweet text was tokenized; repeated punctuation was replaced with the + sign e.g. !!! -> !+; sentence punctuation was split into separate tokens; non-sentence punctuation was removed. Ten-fold cross validation was used in training the classifier. Weights in the graph mentioned in section 2.1 were then updated if two users posted something with a similar sentiment and similar hashtag. 

This research shows a clearly defined process in performing sentiment analysis, particularly the data cleaning step. This process could be adapted for the proposed research \cite{Deitrick:2013}.

\subsection{Other Parameters}

\citeA{Zhang:2012} provided a few formulas to determine similarity in terms of URL, hashtag, following, and retweeting similarity.

URL similarity is given by the same formula as text similarity in section 2.2.1, only using links instead of topics.

Hashtag similarity is given by 

\begin{equation}
sim_{hashtag}(i,j) = \sum_{k=1}^n (1 - \left|{\frac{N_{ik}}{\left|{H_i}\right|} - \frac{N_{jk}}{\left|{H_j}\right|}}\right|)\frac{N_{ik} + N_{jk}}{\left|{H_i}\right| + \left|{H_k}\right|}
\end{equation}

Where $N_{ik}$ is the number of times user $v_i$ used the hashtag $k$ while $H_i$ is the total hashtags used by $v_i$.

Following similarity is given by 

\begin{equation}
sim_{follow}(i,j) = \frac{c_{friend}}{\sqrt{\left|{Friend_i}\right|\left|{Friend_j}\right|}} + \frac{c_{follower}}{\sqrt{\left|{Follower_i}\right|\left|{Follower_j}\right|}}
\end{equation}

$\left|{Friend_i}\right|$ is the total number of users $v_i$ follows. $\left|{Follower_i}\right|$ is the total number of users that follow $v_i$. $c_{friend}$ represents the two users’ common friends. $c_{follower}$ represents the two users’ common followers.

Retweeting similarity is given by 

\begin{equation}
sim_{retweet}(i,j) = \frac{c_{retweet}}{\sqrt{\left|{R_i}\right|\left|{R_j}\right|}} + \frac{n_{ij} + n_{ji}}{\left|{R_i}\right|\left|{R_j}\right|}
\end{equation}

$R_i$ is the number of users whom $v_i$ retweet. $c_{retweet}$ is the number of users both $v_i$ and $v_j$ retweet. $n_{ij}$ is the number of times $v_i$ retweeted $v_j$ and $n_{ji}$ is the inverse case. 

The aggregate similarity is now given by

\begin{equation}
sim(i,j) = \gamma_t sim_{text} + \gamma_u sim_{url} + \gamma_h sim_{hashtag} + \gamma_f sim_{follow} + \gamma_r sim_{retweet}
\end{equation}

With $\gamma_{feature}$ determined in a process described in section 2.3.

This research gives formulas that can be used in the proposed research to measure similarity \cite{Zhang:2012}. 

\section{Community Evaluation Metrics}

\citeA{Zhang:2012} used the average number of mutual following links per user per community(FPUPC) to evaluate their communities. Based on this, appropriate weights for the aggregation were found by first performing their k-means clustering algorithm using only one feature similarity for each of the similarities and extracting the FPUPC. Afterwards, they gave each feature similarity a weight based on the following formula. 

\begin{equation}
w_{feature} = \frac{FPUPC_{feature}}{\sum_{f \in features} FPUPC_f}
\end{equation}

The number of clusters, k, used in the k-means clustering algorithm was also tweaked to get the maximum FPUPC. They concluded that they were successful in generating relatively accurate communities due to the incrementally increasing FPUPC after adjusting the weights.

This provides one possible evaluation metric for the proposed research, as well as a method to provide weights for the feature similarities that the proponents will eventually be using for community detection \cite{Zhang:2012}.
\begin{landscape}
	\begin{table}
		\centering
		\caption {Summary of Review of Related Literature}
		\begin{tabu}{| X[l] | X[l] | X[l] | X[l] | X[l] |}
			\hline
			Reference & Community Detection Algorithms & Sentiment Analysis Model & Other parameters & Community Evaluation \\
			\hline
			\cite{Tang:2010} & Clique percolation method, similarity detection, divisive and agglomerative clustering & & & \\
			\hline
			\cite{Lim:2012:1} & Topic driven community detection, Infomap method, Clique percolation method & & & \\
			\hline
			\cite{Zhang:2012} & k-means clustering & Similarity Formula for Text & Similarity Formula for URL, Hashtag, Follower, and Retweeting & FPUPC metric \\
			\hline
			\cite{Deitrick:2013} & Weighted directed graph, Infomap Algorithm, SLPA & Subjective/Objective, Positive/Negative Naive Bayes Classifier & replies, mentions, retweets, hashtags & \\
			\hline
			\cite{Cao:2015} & Data abstraction design, Hadoop tool & Temporal extent of posts, topics and events, user responses, Statistical Linguistic Sentiment Analysis & & \\
			\hline
		\end{tabu}
	\end{table}
\end{landscape}